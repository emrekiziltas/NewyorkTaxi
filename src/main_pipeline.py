import requests
import pandas as pd
import logging
from datetime import datetime
from pathlib import Path

# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class NYCTaxiDataIngestion:
    """NYC Taxi Trip verilerini Ã§eken ve iÅŸleyen sÄ±nÄ±f"""

    def __init__(self, data_dir='data/raw'):
        """
        Args:
            data_dir: Ham verinin kaydedileceÄŸi dizin
        """
        self.base_url = "https://d37ci6vzurychx.cloudfront.net/trip-data"
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)

    def download_parquet_file(self, year, month, taxi_type='yellow'):
        """
        Belirtilen ay iÃ§in NYC Taxi verisi indir

        Args:
            year: YÄ±l (Ã¶rn: 2024)
            month: Ay (1-12)
            taxi_type: 'yellow', 'green', veya 'fhv'

        Returns:
            DataFrame veya None
        """
        try:
            filename = f"{taxi_type}_tripdata_{year}-{month:02d}.parquet"
            url = f"{self.base_url}/{filename}"
            local_path = self.data_dir / filename

            # EÄŸer dosya zaten varsa, doÄŸrudan oku
            if local_path.exists():
                logger.info(f"Dosya zaten mevcut: {local_path}. Okunuyor...")
                return pd.read_parquet(local_path)

            logger.info(f"Ä°ndiriliyor: {url}")

            # DosyayÄ± indir
            response = requests.get(url, stream=True, timeout=300)
            response.raise_for_status()

            # DosyayÄ± kaydet
            with open(local_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)

            logger.info(f"Ä°ndirme tamamlandÄ±: {local_path}")

            # DataFrame olarak oku
            df = pd.read_parquet(local_path)
            logger.info(f"Veri yÃ¼klendi: {len(df):,} kayÄ±t")

            return df

        except requests.exceptions.HTTPError as e:
            logger.warning(f"Veri indirilemedi (muhtemelen mevcut deÄŸil): {url} - Hata: {e}")
            return None
        except requests.exceptions.RequestException as e:
            logger.error(f"Ä°ndirme sÄ±rasÄ±nda aÄŸ hatasÄ±: {e}")
            return None
        except Exception as e:
            logger.error(f"Veri iÅŸlenirken beklenmeyen hata: {e}")
            return None

    def basic_data_quality_checks(self, df):
        """
        Temel veri kalite kontrolleri

        Args:
            df: Kontrol edilecek DataFrame

        Returns:
            dict: Veri kalite raporu
        """
        if df is None or df.empty:
            return {"status": "failed", "reason": "Empty DataFrame"}

        report = {
            "total_records": len(df),
            "columns": list(df.columns),
            "missing_values": df.isnull().sum().to_dict(),
            "duplicate_records": df.duplicated().sum(),
            "data_types": {col: str(dtype) for col, dtype in df.dtypes.to_dict().items()}
        }

        # Tarih sÃ¼tunlarÄ± iÃ§in kontrol
        date_columns = [col for col in df.columns if 'datetime' in col.lower()]
        if date_columns:
            for col in date_columns:
                try:
                    report[f"{col}_range"] = {
                        "min": str(df[col].min()),
                        "max": str(df[col].max())
                    }
                except Exception as e:
                    logger.warning(f"{col} sÃ¼tunu iÃ§in min/max alÄ±namadÄ±: {e}")
                    report[f"{col}_range"] = "HesaplanamadÄ±"

        logger.info("Veri kalite kontrolÃ¼ tamamlandÄ±")
        return report

    def basic_transformation(self, df):
        """
        Temel veri dÃ¶nÃ¼ÅŸÃ¼mleri

        Args:
            df: DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lecek DataFrame

        Returns:
            DataFrame: DÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ veri
        """
        if df is None or df.empty:
            return df

        logger.info("Temel dÃ¶nÃ¼ÅŸÃ¼mler uygulanÄ±yor...")

        df_transformed = df.copy()

        # Tarih sÃ¼tunlarÄ±nÄ± datetime'a Ã§evir
        date_columns = [col for col in df_transformed.columns if 'datetime' in col.lower()]
        for col in date_columns:
            if not pd.api.types.is_datetime64_any_dtype(df_transformed[col]):
                logger.info(f"{col} sÃ¼tunu datetime formatÄ±na Ã§evriliyor...")
                df_transformed[col] = pd.to_datetime(df_transformed[col], errors='coerce')

        # Negatif deÄŸerleri temizle
        numeric_cols_to_check = [
            'trip_distance', 'fare_amount', 'total_amount',
            'tip_amount', 'tolls_amount'
        ]

        for col in numeric_cols_to_check:
            if col in df_transformed.columns and pd.api.types.is_numeric_dtype(df_transformed[col]):
                initial_count = len(df_transformed)
                df_transformed = df_transformed[df_transformed[col] >= 0]
                removed_count = initial_count - len(df_transformed)
                if removed_count > 0:
                    logger.info(f"{col} sÃ¼tunundaki {removed_count} negatif kayÄ±t kaldÄ±rÄ±ldÄ±.")

        # Tarih bilgilerini ekle
        pickup_col = None
        if 'tpep_pickup_datetime' in df_transformed.columns:
            pickup_col = 'tpep_pickup_datetime'
        elif 'lpep_pickup_datetime' in df_transformed.columns:
            pickup_col = 'lpep_pickup_datetime'

        if pickup_col and pd.api.types.is_datetime64_any_dtype(df_transformed[pickup_col]):
            logger.info(f"{pickup_col} Ã¼zerinden tarih Ã¶zellikleri Ã§Ä±karÄ±lÄ±yor...")
            df_transformed['pickup_hour'] = df_transformed[pickup_col].dt.hour
            df_transformed['pickup_day'] = df_transformed[pickup_col].dt.day
            df_transformed['pickup_weekday'] = df_transformed[pickup_col].dt.dayofweek
            df_transformed['pickup_month'] = df_transformed[pickup_col].dt.month
            df_transformed['pickup_year'] = df_transformed[pickup_col].dt.year
        else:
            logger.warning("pickup_datetime sÃ¼tunu bulunamadÄ± veya datetime formatÄ±nda deÄŸil.")

        logger.info(f"DÃ¶nÃ¼ÅŸÃ¼m tamamlandÄ±: {len(df_transformed):,} kayÄ±t")
        return df_transformed

    def save_to_parquet(self, df, output_path):
        """
        DataFrame'i Parquet olarak kaydet

        Args:
            df: Kaydedilecek DataFrame
            output_path: Ã‡Ä±ktÄ± dosya yolu
        """
        try:
            output_path_obj = Path(output_path)
            output_path_obj.parent.mkdir(parents=True, exist_ok=True)

            # DÃœZELTME: Girinti dÃ¼zeltildi
            df.to_parquet(output_path_obj, index=False, compression='snappy')
            logger.info(f"Veri Parquet olarak kaydedildi: {output_path_obj}")

        except Exception as e:
            logger.error(f"Parquet kaydetme hatasÄ±: {e}")

    def process_month(self, year, month, taxi_type='yellow'):

        logger.info(f"\n--- {year}-{month:02d} verisi iÅŸleniyor ---")

        # Ä°ndir
        df = self.download_parquet_file(year=year, month=month, taxi_type=taxi_type)

        if df is None or df.empty:
            logger.warning(f"{year}-{month:02d} iÃ§in veri bulunamadÄ±.")
            return None

        # DÃ¶nÃ¼ÅŸtÃ¼r
        df_transformed = self.basic_transformation(df)

        if df_transformed is None or df_transformed.empty:
            logger.warning(f"{year}-{month:02d} dÃ¶nÃ¼ÅŸÃ¼m baÅŸarÄ±sÄ±z.")
            return None

        # Kaydet
        output_path = Path(f'data/processed/{taxi_type}_taxi_{year}_{month:02d}_processed.parquet')
        self.save_to_parquet(df_transformed, output_path)

        return output_path

def merge_parquet_files(output_path="data/processed/yellow_taxi_2024_merged.parquet"):
    """Ä°ÅŸlenmiÅŸ 12 aylÄ±k dosyayÄ± birleÅŸtirir"""
    from pathlib import Path
    import pandas as pd
    import logging

    processed_dir = Path("data/processed")
    parquet_files = sorted(processed_dir.glob("yellow_taxi_2024_*_processed.parquet"))

    if not parquet_files:
        logging.error("âš ï¸ HiÃ§ iÅŸlenmiÅŸ Parquet dosyasÄ± bulunamadÄ±.")
        return

    logging.info(f"ğŸ” {len(parquet_files)} dosya bulundu. BirleÅŸtirme baÅŸlÄ±yor...")

    dfs = []
    for f in parquet_files:
        logging.info(f"YÃ¼kleniyor: {f.name}")
        dfs.append(pd.read_parquet(f))

    merged_df = pd.concat(dfs, ignore_index=True)
    logging.info(f"âœ… Toplam {len(merged_df):,} satÄ±r birleÅŸtirildi.")

    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    merged_df.to_parquet(output_path, index=False, compression="snappy")

    logging.info(f"ğŸ’¾ BirleÅŸtirilmiÅŸ veri kaydedildi: {output_path}")
    return output_path


def main():
    """Ana pipeline fonksiyonu"""

    logger.info("=" * 60)
    logger.info("NYC Taxi Data Pipeline BaÅŸlatÄ±lÄ±yor")
    logger.info("=" * 60)

    ingestion = NYCTaxiDataIngestion(data_dir='data/raw')

    processed_files = []
    for year in range (2023, 2025):
        for month in  range (1, 13):
             output_path = ingestion.process_month(year=year, month=month)
             if output_path:
               processed_files.append(output_path)

        if processed_files:
            logger.info(f"\n{len(processed_files)} ay baÅŸarÄ±yla iÅŸlendi.")
        else:
            logger.error("HiÃ§bir veri iÅŸlenemedi.")

        if processed_files:
            logger.info(f"\n{len(processed_files)} ay baÅŸarÄ±yla iÅŸlendi.")
            merged_file = merge_parquet_files()
            if merged_file:
                logger.info(f"ğŸ‰ TÃ¼m aylar baÅŸarÄ±yla birleÅŸtirildi: {merged_file}")
        else:
            logger.error("HiÃ§bir veri iÅŸlenemedi.")


if __name__ == "__main__":
    main()